{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "EXPERIENCE_PATH = './trainingdata/hansolo/young-han.json'\n",
    "PROTECTED_PATH = './trainingdata/hansolo/young-han-protective.json'\n",
    "\n",
    "new_experience = []\n",
    "new_protected = []\n",
    "\n",
    "# loop thru the original jsons\n",
    "# for each object present, do the following:\n",
    "# 1. create an empty dict\n",
    "# 2. add the key \"text\"\n",
    "\n",
    "# with open(EXPERIENCE_PATH, 'r') as f:\n",
    "#     exp = json.load(f)\n",
    "#     for obj in exp:\n",
    "#         #print(obj)\n",
    "#         temp = {'text': []}\n",
    "#         temp['text'].append('<instruction>')\n",
    "#         temp['text'].append(obj['instruction'])\n",
    "#         temp['text'].append('<input>')\n",
    "#         temp['text'].append(obj['input'])\n",
    "#         temp['text'].append('<output>')\n",
    "#         temp['text'].append(obj['output'])\n",
    "#         temp['text'] = ' '.join(temp['text'])\n",
    "#         new_experience.append(temp)\n",
    "#     f.close()\n",
    "# print(len(new_experience))\n",
    "# # create a json file with the new experience\n",
    "# with open('./trainingdata/hansolo/young-han-new.jsonl', 'w') as f:\n",
    "#     for entry in new_experience:\n",
    "#         json.dump(entry, f)\n",
    "#         f.write('\\n')\n",
    "#     # json.dump(new_experience, f)\n",
    "#     f.close()\n",
    "\n",
    "with open(PROTECTED_PATH, 'r') as f:\n",
    "    exp = json.load(f)\n",
    "    for obj in exp:\n",
    "        #print(obj)\n",
    "        temp = {'text': []}\n",
    "        temp['text'].append('<instruction>')\n",
    "        temp['text'].append(obj['instruction'])\n",
    "        temp['text'].append('<input>')\n",
    "        temp['text'].append(obj['input'])\n",
    "        temp['text'].append('<output>')\n",
    "        temp['text'].append(obj['output'])\n",
    "        temp['text'] = ' '.join(temp['text'])\n",
    "        new_protected.append(temp)\n",
    "    f.close()\n",
    "\n",
    "with open('./trainingdata/hansolo/young-han-protective-new.jsonl', 'w') as f:\n",
    "    for entry in new_protected:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "    # json.dump(new_experience, f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 models available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import together\n",
    "from apikey import KEY\n",
    "\n",
    "EXPERIENCE_PATH = './trainingdata/hansolo/young-han-new.jsonl'\n",
    "PROTECTED_PATH = './trainingdata/hansolo/young-han-protective-new.jsonl'\n",
    "\n",
    "together.api_key = KEY\n",
    "\n",
    "model_list = together.Models.list()\n",
    "\n",
    "print(f\"{len(model_list)} models available\")\n",
    "\n",
    "# print the first 10 models on the menu\n",
    "# model_names = [model_dict['name'] for model_dict in model_list]\n",
    "# print(model_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading ./trainingdata/hansolo/young-han-new.jsonl: 100%|██████████| 595k/595k [00:00<00:00, 914kB/s]\n"
     ]
    }
   ],
   "source": [
    "resp = together.Files.upload(file=EXPERIENCE_PATH)\n",
    "file_id = resp[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileTypeError",
     "evalue": "Invalid file supplied. Failed to upload.\nReport:\n {'is_check_passed': False, 'model_special_tokens': 'we are not yet checking end of sentence tokens for this model', 'file_present': 'File found', 'file_size': 'File size 0.0 GB', 'min_samples': 'Processing ./trainingdata/hansolo/young-han-protective-new.jsonl resulted in only 50 samples. Our minimum is 100 samples. '}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileTypeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamzhao/Desktop/characterbot/finetune.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adamzhao/Desktop/characterbot/finetune.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m resp \u001b[39m=\u001b[39m together\u001b[39m.\u001b[39;49mFiles\u001b[39m.\u001b[39;49mupload(file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./trainingdata/hansolo/young-han-protective-new.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adamzhao/Desktop/characterbot/finetune.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m protected_id \u001b[39m=\u001b[39m resp[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/together/files.py:59\u001b[0m, in \u001b[0;36mFiles.upload\u001b[0;34m(self, file, check, model)\u001b[0m\n\u001b[1;32m     57\u001b[0m     report_dict \u001b[39m=\u001b[39m check_json(file)\n\u001b[1;32m     58\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m report_dict[\u001b[39m\"\u001b[39m\u001b[39mis_check_passed\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> 59\u001b[0m         \u001b[39mraise\u001b[39;00m together\u001b[39m.\u001b[39mFileTypeError(\n\u001b[1;32m     60\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid file supplied. Failed to upload.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mReport:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mreport_dict\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     report_dict \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mFileTypeError\u001b[0m: Invalid file supplied. Failed to upload.\nReport:\n {'is_check_passed': False, 'model_special_tokens': 'we are not yet checking end of sentence tokens for this model', 'file_present': 'File found', 'file_size': 'File size 0.0 GB', 'min_samples': 'Processing ./trainingdata/hansolo/young-han-protective-new.jsonl resulted in only 50 samples. Our minimum is 100 samples. '}"
     ]
    }
   ],
   "source": [
    "resp = together.Files.upload(file='./trainingdata/hansolo/young-han-protective-new.jsonl')\n",
    "protected_id = resp[\"id\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
